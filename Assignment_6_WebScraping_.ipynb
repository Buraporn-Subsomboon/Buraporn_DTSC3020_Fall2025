{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Buraporn-Subsomboon/Buraporn_DTSC3020_Fall2025/blob/main/Assignment_6_WebScraping_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_de5Eq4u-tR"
      },
      "source": [
        "# Assignment 6 (4 points) — Web Scraping\n",
        "\n",
        "In this assignment you will complete **two questions**. The **deadline is posted on Canvas**.\n"
      ],
      "id": "H_de5Eq4u-tR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHwamZMu-tX"
      },
      "source": [
        "## Assignment Guide (Read Me First)\n",
        "\n",
        "- This notebook provides an **Install Required Libraries** cell and a **Common Imports & Polite Headers** cell. Run them first.\n",
        "- Each question includes a **skeleton**. The skeleton is **not** a solution; it is a lightweight scaffold you may reuse.\n",
        "- Under each skeleton you will find a **“Write your answer here”** code cell. Implement your scraping, cleaning, and saving logic there.\n",
        "- When your code is complete, run the **Runner** cell to print a Top‑15 preview and save the CSV.\n",
        "- Expected outputs:\n",
        "  - **Q1:** `data_q1.csv` + Top‑15 sorted by the specified numeric column.\n",
        "  - **Q2:** `data_q2.csv` + Top‑15 sorted by `points`.\n"
      ],
      "id": "4PHwamZMu-tX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7DLq9nEu-tZ"
      },
      "outputs": [],
      "source": [
        "1) #Install Required Libraries\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n"
      ],
      "id": "I7DLq9nEu-tZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_A9RuPu-tb"
      },
      "source": [
        "### 2) Common Imports & Polite Headers"
      ],
      "id": "ug_A9RuPu-tb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov8pXh65u-tc"
      },
      "outputs": [],
      "source": [
        "# Common Imports & Polite Headers\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n"
      ],
      "id": "Ov8pXh65u-tc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km0GO7zzu-td"
      },
      "source": [
        "## Question 1 — IBAN Country Codes (table)\n",
        "**URL:** https://www.iban.com/country-codes  \n",
        "**Extract at least:** `Country`, `Alpha-2`, `Alpha-3`, `Numeric` (≥4 cols; you may add more)  \n",
        "**Clean:** trim spaces; `Alpha-2/Alpha-3` → **UPPERCASE**; `Numeric` → **int** (nullable OK)  \n",
        "**Output:** write **`data_q1.csv`** and **print a Top-15** sorted by `Numeric` (desc, no charts)  \n",
        "**Deliverables:** notebook + `data_q1.csv` + short `README.md` (URL, steps, 1 limitation)\n",
        "\n",
        "**Tip:** You can use `pandas.read_html(html)` to read tables and then pick one with ≥3 columns.\n"
      ],
      "id": "km0GO7zzu-td"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_read_table\")\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    TODO: implement cleaning steps.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_clean\")\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    TODO: implement.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_sort_top\")\n"
      ],
      "id": "q1_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d493d3e9-a350-4159-ab30-776068da9a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n",
            "Common helpers loaded.\n",
            "Data successfully written to data_q1.csv\n",
            "\n",
            "--- Top 15 IBAN Country Codes by Numeric (Descending) ---\n",
            "                                                   Country Alpha-2 Alpha-3  Numeric\n",
            "                                                    ZAMBIA      ZM     ZMB      894\n",
            "                                                     YEMEN      YE     YEM      887\n",
            "                                                     SAMOA      WS     WSM      882\n",
            "                                         WALLIS AND FUTUNA      WF     WLF      876\n",
            "                        VENEZUELA (BOLIVARIAN REPUBLIC OF)      VE     VEN      862\n",
            "                                                UZBEKISTAN      UZ     UZB      860\n",
            "                                                   URUGUAY      UY     URY      858\n",
            "                                              BURKINA FASO      BF     BFA      854\n",
            "                                     VIRGIN ISLANDS (U.S.)      VI     VIR      850\n",
            "                            UNITED STATES OF AMERICA (THE)      US     USA      840\n",
            "                              TANZANIA, UNITED REPUBLIC OF      TZ     TZA      834\n",
            "                                               ISLE OF MAN      IM     IMN      833\n",
            "                                                    JERSEY      JE     JEY      832\n",
            "                                                  GUERNSEY      GG     GGY      831\n",
            "UNITED KINGDOM OF GREAT BRITAIN AND NORTHERN IRELAND (THE)      GB     GBR      826\n"
          ]
        }
      ],
      "source": [
        "# --- Q1 write your answer here ---\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n",
        "\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "import io # Import io for using StringIO (good practice for literal strings)\n",
        "\n",
        "url = \"https://www.iban.com/country-codes\"\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    \"\"\"Fetches HTML content from a given URL.\"\"\"\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Flattens MultiIndex headers to a single string header.\"\"\"\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "print(\"Common helpers loaded.\")\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "  tables = pd.read_html(io.StringIO(html))\n",
        "  target_df = None\n",
        "  for df in tables:\n",
        "        if df.shape[1] >= 4:\n",
        "            target_df = df\n",
        "            break\n",
        "\n",
        "  if target_df is None:\n",
        "        raise ValueError(\"Could not find a table with at least 4 columns.\")\n",
        "  target_df = flatten_headers(target_df)\n",
        "  return target_df\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "  col_map = {}\n",
        "  for col in df.columns:\n",
        "    col_lower = col.lower().strip().replace(' ', '')\n",
        "    if 'country' in col_lower:\n",
        "        col_map[col] = 'Country'\n",
        "    elif 'alpha-2' in col_lower or 'alpha2' in col_lower:\n",
        "         col_map[col] = 'Alpha-2'\n",
        "    elif 'alpha-3' in col_lower or 'alpha3' in col_lower:\n",
        "        col_map[col] = 'Alpha-3'\n",
        "    elif 'numeric' in col_lower:\n",
        "        col_map[col] = 'Numeric'\n",
        "\n",
        "  required_cols = ['Country', 'Alpha-2', 'Alpha-3', 'Numeric']\n",
        "  selected_cols = {old: new for old, new in col_map.items() if new in required_cols}\n",
        "  df_clean = df[list(selected_cols.keys())].rename(columns=selected_cols)\n",
        "\n",
        "  if not all(col in df_clean.columns for col in required_cols):\n",
        "        missing = [c for c in required_cols if c not in df_clean.columns]\n",
        "        raise ValueError(f\"Missing required columns after mapping: {missing}\")\n",
        "\n",
        "  for col in ['Country', 'Alpha-2', 'Alpha-3']:\n",
        "    df_clean[col] = df_clean[col].str.strip().str.upper()\n",
        "\n",
        "  df_clean['Alpha-2'] = df_clean['Alpha-2'].str.upper()\n",
        "  df_clean['Alpha-3'] = df_clean['Alpha-3'].str.upper()\n",
        "\n",
        "  df_clean['Numeric'] = pd.to_numeric(\n",
        "        df_clean['Numeric'].astype(str).str.strip(),\n",
        "        errors='coerce'\n",
        "    ).astype('Int64')\n",
        "\n",
        "  df_clean.dropna(subset=['Alpha-2', 'Alpha-3', 'Numeric'], inplace=True)\n",
        "  return df_clean\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    df = df.sort_values(by='Numeric', ascending=False)\n",
        "    return df.head(top)\n",
        "\n",
        "try:\n",
        "    html_content = fetch_html(url)\n",
        "    raw_df = q1_read_table(html_content)\n",
        "    cleaned_df = q1_clean(raw_df.copy())\n",
        "    csv_filename = \"data_q1.csv\"\n",
        "    cleaned_df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Data successfully written to {csv_filename}\")\n",
        "\n",
        "    top_items_df = q1_sort_top(cleaned_df, top=15)\n",
        "    print(\"\\n--- Top 15 IBAN Country Codes by Numeric (Descending) ---\")\n",
        "    print(top_items_df.to_string(index=False))\n",
        "\n",
        "except requests.exceptions.HTTPError as e:\n",
        "    print(f\"Error fetching URL: HTTP {e.response.status}\")\n"
      ],
      "id": "q1_skeleton_answer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmefu--_u-tg"
      },
      "source": [
        "## Question 2 — Hacker News (front page)\n",
        "**URL:** https://news.ycombinator.com/  \n",
        "**Extract at least:** `rank`, `title`, `link`, `points`, `comments` (user optional)  \n",
        "**Clean:** cast `points`/`comments`/`rank` → **int** (non-digits → 0), fill missing text fields  \n",
        "**Output:** write **`data_q2.csv`** and **print a Top-15** sorted by `points` (desc, no charts)  \n",
        "**Tip:** Each story is a `.athing` row; details (points/comments/user) are in the next `<tr>` with `.subtext`.\n"
      ],
      "id": "rmefu--_u-tg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q2 Skeleton (fill the TODOs) ---\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    TODO: implement with BeautifulSoup on '.athing' and its sibling '.subtext'.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_parse_items\")\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    TODO: cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_clean\")\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N. TODO: implement.\"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_sort_top\")\n"
      ],
      "id": "q2_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q2_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19a2f9e-ee80-4033-df01-2fbddde36543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n",
            "Common helpers loaded.\n",
            "Data successfully written to data_q2.csv\n",
            "\n",
            "--- Top 15 by points ---\n",
            " rank                                                                          title  points  comments           user\n",
            "    2                                               Solarpunk is happening in Africa     597       301        JoiDegn\n",
            "    9            New gel restores dental enamel and could revolutionise tooth repair     360       151   CGMthrowaway\n",
            "   22               I was right about dishwasher pods and now I can prove it [video]     316       202  hnaccount_rng\n",
            "   17                                           The shadows lurking in the equations     258        81         calebm\n",
            "    4                                  Dillo, a multi-platform graphical web browser     245        91   nazgulsenpai\n",
            "    5 ChatGPT terms disallow its use in providing legal and medical advice to others     236       222 randycupertino\n",
            "   27 SPy: An interpreter and compiler for a fast statically typed variant of Python     235       110        og_kalu\n",
            "   18                                  NY school phone ban has made lunch loud again     222       168        hrldcpr\n",
            "   12                                               Why aren't smart people happier?     215       332            zdw\n",
            "   21                                 An eBPF Loophole: Using XDP for Egress Traffic     210        69   loopholelabs\n",
            "   15                                        Carice TC2 – A non-digital electric car     189       143      RubenvanE\n",
            "   26                                  App Store web has exposed all its source code     188        53        redbell\n",
            "   20  Vacuum bricked after user blocks data collection – user mods it to run anyway     185        58 toomanyrichies\n",
            "   13                                              Ruby and Its Neighbors: Smalltalk     176        98     jrochkind1\n",
            "   19                                                               Radiant Computer     172       130      beardicus\n"
          ]
        }
      ],
      "source": [
        "# Q2 — Write your answer here\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from urllib.parse import urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL = \"https://news.ycombinator.com/\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "        \"(KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n",
        "\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    items = []\n",
        "\n",
        "    for row in soup.find_all(\"tr\", class_=\"athing\"):\n",
        "        data = {}\n",
        "\n",
        "        rank_tag = row.find(\"span\", class_=\"rank\")\n",
        "        data[\"rank\"] = rank_tag.get_text(strip=True) if rank_tag else None\n",
        "\n",
        "        titleline = row.find(\"span\", class_=\"titleline\")\n",
        "        a = titleline.find(\"a\") if titleline else None\n",
        "        data[\"title\"] = a.get_text(strip=True) if a else None\n",
        "        href = a.get(\"href\") if a else None\n",
        "        data[\"link\"] = urljoin(URL, href) if href else None\n",
        "\n",
        "        sub = row.find_next_sibling(\"tr\")\n",
        "        points_text = user_text = comments_text = None\n",
        "\n",
        "        if sub:\n",
        "            subtext_td = sub.find(\"td\", class_=\"subtext\")\n",
        "            if subtext_td:\n",
        "                score = subtext_td.find(\"span\", class_=\"score\")\n",
        "                points_text = score.get_text(strip=True) if score else None\n",
        "\n",
        "                user_tag = subtext_td.find(\"a\", class_=\"hnuser\")\n",
        "                user_text = user_tag.get_text(strip=True) if user_tag else None\n",
        "\n",
        "                comment_link = None\n",
        "                for a_tag in subtext_td.find_all(\"a\"):\n",
        "                    txt = a_tag.get_text(strip=True).lower()\n",
        "                    if \"comment\" in txt or txt == \"discuss\":\n",
        "                        comment_link = a_tag\n",
        "                comments_text = comment_link.get_text(strip=True) if comment_link else None\n",
        "\n",
        "        data[\"points\"] = points_text\n",
        "        data[\"user\"] = user_text\n",
        "        data[\"comments\"] = comments_text\n",
        "\n",
        "        items.append(data)\n",
        "\n",
        "    return pd.DataFrame(items)\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for col in [\"rank\", \"points\", \"comments\"]:\n",
        "        df[col] = df[col].apply(safe_int_extract).astype(int)\n",
        "    for col in [\"title\", \"link\", \"user\"]:\n",
        "        df[col] = df[col].fillna(\"\").astype(str).str.strip()\n",
        "    return df\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    return df.sort_values(by=\"points\", ascending=False).head(top).reset_index(drop=True)\n",
        "\n",
        "try:\n",
        "    html = fetch_html(URL)\n",
        "    raw = q2_parse_items(html)\n",
        "    cleaned = q2_clean(raw)\n",
        "    csv_filename = \"data_q2.csv\"\n",
        "    cleaned.to_csv(csv_filename, index=False)\n",
        "    print(f\"Data successfully written to {csv_filename}\")\n",
        "\n",
        "    top15 = q2_sort_top(cleaned, 15)\n",
        "    print(\"\\n--- Top 15 by points ---\")\n",
        "    print(top15[[\"rank\", \"title\", \"points\", \"comments\", \"user\"]].to_string(index=False))\n",
        "\n",
        "except requests.exceptions.HTTPError as e:\n",
        "    print(f\"Error fetching URL: HTTP {e.response.status_code}. Cannot continue.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}. Check your network connection or the site structure.\")\n"
      ],
      "id": "q2_skeleton_answer"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}